---
title: "MA5810 Introduction to Data Mining - Assignment 2"
author: "By Sean O'Rourke (13984624)"
date: "Due 8 August 2021"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)
```
***

## Question 1

```{r q1 prep}
##################
### Question 1 ###
##################

## Clear environment
rm(list = ls()) # removes all variables
if(!is.null(dev.list())) dev.off() # clear plots
cat("\014") # clear console

## Import required packages
library(caret, warn.conflicts = FALSE, quietly = TRUE) # handy ml package, data splitting, training ect ect
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE) # handy for data prep
library(reshape2, warn.conflicts = FALSE, quietly = TRUE) # handy for melt() - plotting data frames with ggplot
library(alookr, warn.conflicts = FALSE, quietly = TRUE) # for removing correlated variables
library(ggplot2, warn.conflicts = FALSE, quietly = TRUE) # plotting
library(ggpubr, warn.conflicts = FALSE, quietly = TRUE) # added plotting function
library(DataExplorer, warn.conflicts = FALSE, quietly = TRUE) # quick exploratory vis
library(corrplot, warn.conflicts = FALSE, quietly = TRUE) # plotting corrmatrix
library(ROCR, warn.conflicts = FALSE, quietly = TRUE) # for ROC curves
library(ROCit, warn.conflicts = FALSE, quietly = TRUE) # for ROC curves


file <- 'Breast_cancer.csv' # store the path to the source data
rawData <- read.csv(file, header = TRUE, stringsAsFactors = TRUE) # import source data, in this case the data file has headers and strings will be used a factors
names(rawData) <- c("id", "diagnosis", "radius", "texture","perimeter", "area", "smoothness", "compactness", "concavity", "concave.points", "symmetry",  "fractal_dimension") # set column names to meaningful titles
rawData <- within(rawData, rm("id")) # remove variable ID as it is simply a record identification

```
The data was imported directly from the UCI Data Repository using read.csv() and the HTML link with strings.as.factors = TRUE so as categorical data becomes a factor. Some very basic data exploration was then undertaken to ensure the data was imported properly. The outputs are shown in Appendix 1, Figures 1 to 3.

```{r q1 initial vis}
## initial exploratory vis, stored as variables to display in appendix 1
a1_1_intro <- introduce(rawData) # provides introductory summary, good to confirm variable types, missing data ect
a1_2_head <- head(rawData) # handy to have a look at actual data
a1_3_dataForAppendix <- rawData
```
The initial data exploration confirms that we have no missing values, the dependent variable "diagnosis" is a factor with 2 levels and the remaining variables are numeric. The data was then prepared for logistic regression with the following 3 steps.

* Standardization of predictor variables so as they contribute to the model evenly (conformation of the output is shown in Appendix 1, Figure 4),
* Removal of any correlated predictor variables,
* Visual inspection of the relationship between predictors and the log odds of the outcome,
* Visual inspection of variables broken down into diagnosis class to explore normality and gain an empirical understanding of the prediction power of each variable,
* Visual inspection for any observations which may be overly influential in the model by plotting cooks distance for each observation.

```{r standardise data}
## standardize data so as it has mean 0 and variance 1
rD_stz <- preProcess(rawData[ ,-1], method = c("scale", "center")) # set parameters for scaling data
rawData_stz <- predict(rD_stz, rawData) # scale data
a1_4_sum_after_stz <- summary(rawData_stz) # save summary to display in appendix

```

```{r correlation, include = TRUE, out.width="50%", out.extra='style="float:right; padding:10px"'}
## plot correlation matrix to inspect for correlated variables
corMatrix <- round(cor(rawData_stz[ ,-1], method = "pearson"), 2) # calculate correlation matrix, using pearson
corrplot.mixed(corMatrix, order = "AOE") # plot correlation matrix

```
Arbitrarily a correlation coefficient of between 0.8 and 0.9 is often used as a threshold. From inspecting the above figure it can be seen the following variables will need to be reviewed:

* Radius
* Area
* Perimeter
* Compactness
* Concavity
* Concave points

Little domain knowledge is required to understand that area, perimeter and radius should be closely related and thus retaining only one is a prudent step to take. Without a good understanding of the domain, full confidence in how to deal with Compactness, Concavity and Concave Points is a more difficult. To asses which correlated variables should be kept the variables will be inspected for their suitability to wrt the assumption of a linear response to the log odds of the response variable and their distribution frequencies. 

```{r log odds and linerarity}
## rough and ready glm and calculation of log odds for plots and predictor assessment
logistic_initial <- glm(diagnosis ~ . , data = rawData_stz, family = binomial(link = "logit")) # fit a glm model, binomial as we have only 2 responses classes, using the logit link function
probs_initial <- predict(logistic_initial, type = "response") # use model to calculate the probabilities of the positive outcome 
predictors <- names(rawData_stz[ ,-1]) # names of predictor variables
odds <- rawData_stz %>% mutate(logit = log(probs_initial / (1 - probs_initial))) # create new column with the log odds for each case
odds <- melt(odds[ ,-1], id.vars = "logit") # melt data for plotting

```
```{r linear plots, include = TRUE, out.width="50%", out.extra='style="float:left; padding:10px"' }
## Plot predictors vs log odds to check linearity assumption
ggplot(odds, aes(logit, value)) + geom_point(size = 0.5, alpha = 0.5) + facet_wrap(~variable) + geom_smooth(formula = y ~ x, method = "loess") + ggtitle("Predictor Variables vs Log Odds of Response Variable") # create a grid of plots with log odds on x axis and the value on the y

```
```{r dist_plots, include = TRUE, out.width="50%", out.extra='style="float:right; padding:10px"'}
## frequency distributions to understand distribution of predictors by response class
ggplot(data = melt(rawData_stz, id.var = "diagnosis" ), mapping = aes(x = value, fill = diagnosis)) + geom_density(alpha = 0.5) + facet_wrap(~variable, scales = "free") + ggtitle("Density distribution by diagnosis for all predictors")

```
  
```{r drop corelated variables}
## Replot correlation matrix to ensure suitab;e, save to display in appendix 1
rawData_drop <- within(rawData_stz, rm("perimeter", "area", "compactness", "concave.points")) # drop correlated variables
a1_5_cor <- round(cor(rawData_drop[ ,-1]), 2) # create correlation matrix
```
Perimeter and Area are both function of radius, therefore it follows, and can be seen that the errors they exhibit are the same simply scaled. Interestingly a more pronounced parabolic deviation from linearity could be expected for area (since it should be defined by radius^2). So as any no linearity or other error is not scaled the radius variable will be selected and perimeter and area excluded. The other variables which were found to be heavily correlated were Compactness, Concavity and Concave Points. All 3 were heavily correlated so only one can reasonably be retained. Concavity clearly has the most linear response to the log odds so it will be retained. While not an explicit assumption of the logistic model, Concavity has the most normal distribution and a clear separation between response classes as shown in the density distributions. Texture, Smoothness, Symmetry and Fractal Dimension also exhibit varying degrees of non linearity however without good cause to exclude these variables they will be retained. After the 3 discussed variables were dropped correlation was again plotted. The results are shown in Appendix 1, Figure 5. The highest correlation coefficient of the retained variables is 0.68, which is generally considered acceptable.\
```{r influence}
## rough and ready glm for predictor assessment
logistic_influence <- glm(diagnosis ~ . , data = rawData_drop, family = binomial(link = "logit")) # fit a glm model, binomial as we have only 2 responses classes, using the logit link function
```

```{r cooks plot, include = TRUE, out.width="65%", out.extra='style="float:right; padding:10px"' }
## plot cooks's distance
plot(logistic_influence, which = 4, id.n = 5) # plot cooks distance
```
The final step in assessment of the data for suitability is to check that no single observation is overly influential and thus could be considered an outlier.This assessment is made by calculating Cook's Distance, which
asses the effect of deleting any given observation. The logistic model will be fitted to the retained predictors and Cook's distance plotted for assessment. The plot shows that while some observations are reletevly strong, they do not exhibit a large Cooks's number in absolute terms. Without domain knowledge on how to address any outliers detected further investigation is not warranted.\
It is now clear that the data is suitably close to meeting the assumptions of Logistic regression and a model can be fitted and assessed on training and test data sets. As such, the data was then split into a single test training split and the logistic regression model was fitted. The createDataPartition() from caret was used to create a stratified sample along the response variable. The glm() from base r was then used to fit the logistic regression model. The following key parameters were used. As there are no hyperparameters to tune there was no need for cross validation within the training data. 

* 80% / 20% training test split, with stratified sampling across the diagnosis variable,
* A "binomial" function was selected as the response has only 2 levels,
* the "logit" function was selected as the link function.

```{r housekeeping}
# house keeping
modelData <- rawData_drop # move data into modelData data frame
rm(corMatrix, odds, rD_stz, file, predictors, probs_initial, rawData, rawData_drop, rawData_stz, logistic_influence, logistic_initial) # remove variables that are not required.

```

```{r final model}
### Prepare and fit final model
## Create test / training split
train_index <- createDataPartition(modelData$diagnosis, p=0.8, list = FALSE) # returns numerical vector of the index of the observations to be included in the training set
testData <- modelData[-train_index, ] # create data.frame of test data
trainingData <- modelData[train_index, ] # create data frame of training predictors

## fit logistic regression model
bc_logistic <- glm(diagnosis ~ . , data = trainingData, family = binomial(link = "logit")) # fit logistic regresion model

## return some information for assessing the model
prob_train <- predict(bc_logistic, type = "response") # calculate probabilities of each case in the training data being "M"
prob_train <- ifelse(prob_train > 0.5, "M", "B") # convert probability into classification assuming .5 descion boundary
confMatrix_train <- confusionMatrix(as.factor(prob_train), trainingData$diagnosis) # create confusion matrix summarizing accuracy on training data

```

```{r model assesment}
## crsave assesment values for displaying in appendix
ROCpred <- prediction(predict(bc_logistic, type = "response"), trainingData$diagnosis)
a1_6_roc <- performance(ROCpred, "tpr", "fpr") # save values for roc curve
a1_7_1_dev <- bc_logistic$deviance # save deviance
a1_7_2_nullDev <- bc_logistic$null.deviance # save null deviance
a1_7_3_aic <- bc_logistic$aic # save aic 
a1_8 <- confMatrix_train # save confusion matrix

```
Now that the model has been fitted we can assess how well the model fits the data and determine how to best use the model to make predictions. Firstly to asses how well the model fits the training data we will consider the following 3 indicators:

* Empirical review of the ROC curve,
* Residual deviance and AIC,
* The performance of the model at making predictions with a threshold of 0.5.

The output from the above 3 items is attached In appendix 1 figures 6 to 8 respectively. It can be seen that there is a significant reduction in the deviance from a null score of ~600 to a fitted score of ~120. Further the AIC score of 140 is reasonable. The model can also be seen to perform well from a visual assessment of the ROC curve showing the model differs significantly from a simple chance machine that would return a straight line from 0,0 to 1,1. Finally the confusion matrix shows the model has a meaningful ability to predict that the case is malignant or benign. Importantly it should be noted that the "b" or benign class is considered positive. While in many situations overall model accuracy may be considered the best measure of model accuracy, in this situation, considering diagnosed cancer is significantly worse than investigating a benign growth further the model will be adjusted to better capture this. As the model is framed with the benign case as the positive class the rate of "true positives" needs to be maximized, ie minimise the number of false positives (malignant cases classified as benign). This is also known as maximizing the sensitivity of the model. Referring back to the ROC curve in Appendix 1 it can be seen that the rate of false positives fall considerably, with a reasonable drop in true positives unto a probability threshold of 0.8. to further explore this the adjacent figure shows the specificity of the model vs the cut off probability threshold.

```{r threshold assesment, include = TRUE, out.width="75%", out.extra='style="float:right; padding:10px"'}
## Create plot of positive predictive power
PPV <- measureit(score = bc_logistic$fitted.values, class = bc_logistic$y, measure = c("PPV")) # creates object summarizing the PPV of the model for different cutoffs
PPV_plotDF <- data.frame(PPV$Cutoff, PPV$PPV) # convert to df for ggplot
names(PPV_plotDF) <- c("Probability Threshold", "Positive Predictive Power") # name data frame columns
ppv_plot <- ggplot(na.omit(PPV_plotDF), aes(`Probability Threshold`, `Positive Predictive Power`)) + geom_line(size = 1, colour = "red") + geom_hline(aes(yintercept=0.995), colour = "blue")+ geom_text(aes( 0, 0.995, label = 0.995, vjust = -1), size = 3, colour = "blue") + geom_vline(aes(xintercept=0.8), colour = "blue")+ geom_text(aes( .8, 0, label = 0.8, hjust = -1), size = 3, colour = "blue") + theme_light() + ggtitle("Positive Predictive power Vs Probability Threshold") # define plot
ppv_plot
```
Now that the model has been refined to suit the domain we will asses it performance on the test data set using a probability thresholds of 0.8 and 0.5 to investigate the consequence of our decision to adopt 0.8. The probability thresholds simply refer to the likelihood that the observation will be benign (or more generally belong to the positive class of the model). The confusion matrix tables for probability 0.5 and probability 0.8 are shown in appendix 1 figures 9 and 10. These tables show that with a probability of 0.8 no cases are identified as benign that are malignant, where as if 0.5 was used 2 cases would have been miss classified. The trade off for this accuracy is that one case was miss classified as malignant when it was actually benign. The bellow stats confirm this with a sensitivity of 1 (100%) for the 0.8 probability and a reduction of 0.02 or 2% in specificity.
```{r test data assesment p  0.8, include = TRUE}
prob_test <- predict(bc_logistic, newdata = testData, type = "response") # calculate probabilities of each case in the test data being "M"
prob_test_5 <- ifelse(prob_test > 0.5, "M", "B") # convert probability into classification assuming .5 descion boundary
prob_test_8 <- ifelse(prob_test > 0.8, "M", "B") # convert probability into classification assuming .5 descion boundary
confMatrix_test_5 <- confusionMatrix(as.factor(prob_test_5), testData$diagnosis) # create confusion matrix summarizing accuracy on training data
confMatrix_test_8 <- confusionMatrix(as.factor(prob_test_8), testData$diagnosis) # create confusion matrix summarizing accuracy on training data
a1_9_cor5 <- confMatrix_test_5$table # save confusion matrix table for appendix
a1_10_cor8 <- confMatrix_test_8$table # save confusion matrix for appendix
confMatrix_test_5$byClass # return accuracy by class
confMatrix_test_8$byClass # return accuracy by class
```
The logistic regression model can also be used to assess the importance of each predictor, or how much each predictor contributes to the model. This is stated as a p-value and the null hypothesis is framed that the variable is not significant to the model. Thus predictors which have small p-values, or a low chance of the null hypothesis being true are considered significant. As in many statistical applications, without further investigation, a p- value < 0.05 or a 95% chance the null hypothesis is false is the usual boundary for a variable to be considered significant. The p values of the predictors is shown in the below output. It can be seen that the variables Radius, Texture, Smoothness, Concavity are all extremely significant to the model. The variable Fractal Dimension is some what significant and Symmetry is not significant. It is reasonable to assume that these levels of significance reflect how important each predictor is in identifying the odds of having cancer. When reviewing this with the frequency distributions plotted in the exploratory analysis it is consistent that the predictors with minimal overlap between classes are more powerful predictors.

```{r p-values, include = TRUE}
## output p values 
summary(bc_logistic) # p values

```
While only radius was included in the model, as radius and area were so heavily correlated (0.99), it is reasonable to assume that they would both have a similar effect. This is a good assumption as variables with such high levels of correlation can be considered to be essentially the same variable. This is supported by the fact that both variables presented nearly identically in the exploratory visualization, of particular importance in the distribution frequencies. 

***






***
## Appendix 1

### Figure 1
```{r Apendix 1_1, include = TRUE}
a1_1_intro

```

### Figure 2
```{r Apendix 1_2, include = TRUE}
a1_2_head

```

### Figure 3
```{r Apendix 1_3, include = TRUE}
str(a1_3_dataForAppendix)

```

### Figure 4
```{r Apendix 1_4, include = TRUE}
a1_4_sum_after_stz

```

### Figure 5
```{r Apendix 1_5, include = TRUE}
corrplot.mixed(a1_5_cor, order = "AOE")

```

### Figure 6
```{r Apendix 1_6, include = TRUE}
plot(a1_6_roc, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.2), text.adj=c(-0.2,1.7)) + title(main = "ROC Curve")

```

### Figure 7
```{r Apendix 1_7, include = TRUE}
a1_7_1_dev
```

### Figure 8
```{r Apendix 1_8, include = TRUE}
a1_8

```

### Figure 9
```{r Apendix 1_9, include = TRUE}
a1_9_cor5

```

### Figure 10â€¢
```{r Apendix 1_10, include = TRUE}
a1_10_cor8

```



